<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>recops/recops/overview/glossary/</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><link rel=stylesheet href=https://forensic-toolkit.github.io/recops/hugo-theme-console/css/terminal-0.7.2.min.css><link rel=stylesheet href=https://forensic-toolkit.github.io/recops/hugo-theme-console/css/animate-4.1.1.min.css><link rel=stylesheet href=https://forensic-toolkit.github.io/recops/hugo-theme-console/css/console.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><meta property="og:title" content="Glossary"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://forensic-toolkit.github.io/recops/overview/glossary/"><meta property="article:published_time" content="2022-06-02T00:00:00+00:00"><meta name=twitter:title content="Glossary"><meta name=twitter:description content="This glossary is a list of definitions of terms and concepts relevant to the recops project, a critical investigation for the analysis and critique of facial recognition technologies. Some of these terms may be familiar to you, others you may not have come across before. In fact, many of these terms are entire fields of study on their own. The intention isn’t to give a comprehensive definition, but rather to pique your interest to explore further."></head><body class=terminal><div class=container><div class=terminal-nav><header class=terminal-logo><div class="logo terminal-prompt"><a href=https://forensic-toolkit.github.io/recops/ class="no-style site-name">recops</a>:~#
<a href=https://forensic-toolkit.github.io/recops/overview>overview</a>/<a href=https://forensic-toolkit.github.io/recops/overview/glossary>glossary</a>/</div></header><nav class=terminal-menu><ul vocab="https://schema.org/" typeof="BreadcrumbList"><li><a href=https://forensic-toolkit.github.io/recops/about/ typeof="ListItem">about/</a></li><li><a href=https://forensic-toolkit.github.io/recops/overview/ typeof="ListItem">overview/</a></li><li><a href=https://forensic-toolkit.github.io/recops/documentation/ typeof="ListItem">documentation/</a></li></ul></nav></div></div><div class="container animated zoomIn fast"><h1>Glossary</h1><p>This glossary is a list of definitions of terms and concepts relevant to the <a href=../../about/><code>recops</code> project</a>, a critical investigation for the analysis and critique of <a href=#f>facial recognition</a> technologies. Some of these terms may be familiar to you, others you may not have come across before. In fact, many of these terms are entire fields of study on their own. The intention isn’t to give a comprehensive definition, but rather to pique your interest to explore further.</p><p>Below are brief definitions of terms and concepts in alphabetical order. (<em>NOTE: not all letters of the alphabet are listed if they do not contain relevant enough terms.</em>) Contents:</p><table><thead><tr><th><strong><a href=#a>A</a></strong></th><th><strong><a href=#b>B</a></strong></th><th><strong><a href=#c>C</a></strong></th><th><strong><a href=#d>D</a></strong></th><th><strong><a href=#e>E</a></strong></th><th><strong><a href=#f>F</a></strong></th><th><strong><a href=#g>G</a></strong></th><th><strong><a href=#h>H</a></strong></th><th><strong><a href=#i>I</a></strong></th><th><strong><a href=#j>J</a></strong></th><th><strong>K</strong></th><th><strong><a href=#l>L</a></strong></th><th><strong><a href=#m>M</a></strong></th><th><strong><a href=#n>N</a></strong></th><th><strong>O</strong></th><th><strong><a href=#p>P</a></strong></th><th><strong>Q</strong></th><th><strong><a href=#r>R</a></strong></th><th><strong><a href=#s>S</a></strong></th><th><strong><a href=#t>T</a></strong></th><th><strong><a href=#u>U</a></strong></th><th><strong><a href=#v>V</a></strong></th><th><strong>W</strong></th><th><strong>X</strong></th><th><strong>Y</strong></th><th><strong>Z</strong></th></tr></thead></table><hr><h2 id=a>A</h2><ul><li><p><strong>algorithm:</strong> in <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>, a traditional algorithm is a precise set of rules or instructions that tell a computer how to solve a problem. With a traditional algorithm, to complete the task the computer must follow the steps in the order they’re laid out by the programmer. By contrast, a <a href=#m>machine learning (ML)</a> algorithm creates the rules or ideal steps itself, by experimenting with the task and learning as it goes.</p></li><li><p><strong>artificial intelligence (AI):</strong> artificial intelligence or AI for short. Any intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In <a href=https://en.wikipedia.org/wiki/Computer_science>computer science</a>, AI research is defined as the study of <em>&ldquo;intelligent agents&rdquo;</em>: any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term artificial intelligence is applied when a machine mimics cognitive functions that humans associate with other human minds, such as learning and problem solving.</p></li><li><p><strong>accuracy:</strong> accuracy expresses how precise the <a href=#m>machine learning (ML)</a> model is based on the percentage of correct predictions. The number of correct predictions divided by the number of total predictions allows calculating the accuracy of the model.</p></li><li><p><strong>active learning:</strong> active learning represents a training algorithm that interactively queries the information source to label fresh data points with the intended outputs. The active learning algorithm randomly chooses the data points it learns from, which is particularly valuable when <a href=#l>labeled</a> examples are scarce or expensive to obtain.</p></li><li><p><strong>annotation:</strong> the process of <a href=#l>labeling</a> the input data in preparation for <a href=#a>AI</a> training. In <a href=#c>computer vision</a>, the input images and video must be annotated according to the task you want the AI <a href=#m>model</a> to perform. For example, if you want the model to perform <a href=https://en.wikipedia.org/wiki/Image_segmentation>image segmentation</a>, the annotations must include the location and shape of each object in the image.</p></li><li><p><strong>annotation format:</strong> the particular way of encoding an annotation. There are many ways to describe a <a href=#b>bounding box&rsquo;s</a> size and position (JSON, XML, TXT, etc) and to delineate which annotation goes with which image.</p></li><li><p><strong>annotation group:</strong> describes what types of object you are identifying. For example, <em>&ldquo;Chess Pieces&rdquo;</em> or <em>&ldquo;Vehicles&rdquo;</em>. Classes (eg <em>&ldquo;rook&rdquo;</em>, <em>&ldquo;pawn&rdquo;</em>) are members of an annotation group.</p></li><li><p><strong>automation bias:</strong> when a human decision maker favors recommendations made by an automated decision-making system over information made without automation, even when the automated decision-making system makes errors.</p></li><li><p><strong>attribute:</strong> synonym for <a href=#f>feature</a>. In fairness, attributes often refer to characteristics pertaining to individuals.</p></li></ul><hr><h2 id=b>B</h2><ul><li><p><strong>biometrics:</strong> a measurable physical characteristic or personal behavioral trait used to recognize the identity, or verify the claimed identity, of an applicant. <a href=#f>Facial images</a>, <a href=https://en.wikipedia.org/wiki/Fingerprint>fingerprints</a>, and <a href=https://en.wikipedia.org/wiki/Iris_recognition>iris scan</a> samples are all examples of biometrics.</p></li><li><p><strong>bias:</strong> bias is stereotyping, prejudice, or preference towards particular items (data points) over others. In <a href=#m>machine learning (ML)</a>, bias is considered a systematic error that occurs in the <a href=#t>training set</a> or <a href=#m>machine learning (ML)</a> <a href=#m>model</a> when the <a href=#a>algorithm</a> outcome is distorted in favor of or against a certain idea. Bias impacts data collection and interpretation, system design, and users&rsquo; engagement with a system.</p></li><li><p><strong>benchmark dataset:</strong> benchmark dataset is a <a href=#t>test dataset</a>. But typically refers to <a href=#d>dataset</a> shared with other researchers or organizations for the purpose of comparing algorithmic performance.</p></li><li><p><strong>bounding box:</strong> a bounding box is a rectangular area around an object on a digital image, that’s typically described by its (x, y) coordinates around an area of interest.</p></li></ul><hr><h2 id=c>C</h2><ul><li><p><strong>convolutional neural networks (CNN, or ConvNet):</strong> are by far the most popular <a href=#n>neural networks</a> for <a href=#c>computer vision (CV)</a> and <a href=https://en.wikipedia.org/wiki/Image_analysis>image analysis</a> tasks due to their ability to extract features and detect patterns via hidden convolutional layers within the network.</p></li><li><p><strong>classification:</strong> classification is a <a href=#s>supervised learning</a> technique that aims to categorize the target variables. For instance, detecting whether an email is a spam or not is a classification task. It is also called a binary classification since the target variable has only two possible values, spam or not. If the target variable contains more than two values (i.e., classes), it is known as multi-class classification.</p></li><li><p><strong>classification model:</strong> a type of <a href=#m>model</a> that distinguishes among two or more discrete <a href=#c>classes</a>. For example, a natural language processing classification model could determine whether an input sentence was in French, Spanish, or Italian.</p></li><li><p><strong>clustering:</strong> clustering allows for grouping data points based on their similarity in one cluster. Unlike classification, the data points in clustering do not have <a href=#l>labels</a>, hence it’s an <a href=#u>unsupervised learning</a> technique.</p></li><li><p><strong>computer vision (CV):</strong> computer vision or CV for short – is a field of <a href=#a>artificial intelligence (AI)</a> that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs — and take actions or make recommendations based on that information.</p></li><li><p><strong>class:</strong> a class is a label that gives information about the instance. To illustrate, when <a href=#a>annotating</a> images of apples, we would annotate the apples and assign each of them to the class <em>“Apple”</em>.</p></li><li><p><strong>confirmation bias:</strong> the tendency to search for, interpret, favor, and recall information in a way that confirms one&rsquo;s preexisting beliefs or hypotheses. <a href=#m>Machine learning (ML)</a> developers may inadvertently collect or label data in ways that influence an outcome supporting their existing beliefs. Confirmation bias is a form of <a href=#i>implicit bias</a>.</p></li><li><p><strong>central processing unit (CPU):</strong> the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control and input/output operations specified by the instructions.</p></li></ul><hr><h2 id=d>D</h2><ul><li><p><strong>deep learning:</strong> deep learning is a subset of <a href=#m>machine learning (ML)</a>, inspired by how the human brain works. Deep Learning relies on <a href=#a>algorithms</a> by which a machine/computer can teach itself how to do something based on ‘looking at’ and learning about a huge <a href=#d>dataset</a> of examples.</p></li><li><p><strong>deep convolution neural network (DCNN):</strong> refers to using convolutional network layers to learn visual features. A convolution is an image (data matrix) operation that convolves (combines) nearby visual information using a transformation function. For example, using and edge filter in a graphics editing program is a convolution matrix. Another example would be an unsharp mask, as shown in this interactive <a href=https://generic-github-user.github.io/Image-Convolution-Playground/src/>convolutional demo</a>. Another helpful example is <a href=https://setosa.io/ev/image-kernels/>https://setosa.io/ev/image-kernels/</a>. A DCNN uses multiple layers of convolutions to understand visual features and concepts within images.</p></li><li><p><strong>data:</strong> information of any kind. It could be images, text, sound, or tabular.</p></li><li><p><strong>data set or dataset:</strong> dataset refers to a collection of images with associated <a href=#m>metadata</a> used for training, validating, testing a <a href=#c>computer vision (CV)</a> algorithm. Typically the dataset comprises a compressed ZIP file with folders of JPEGs and JSON or XML metadata text files. The dataset of images is often divided into 3 subsets called <a href=#t>training</a>, <a href=#v>validation</a>, and <a href=#t>test</a>.</p></li><li><p><strong>data exploration:</strong> is a critical step in <a href=#a>artificial intelligence (AI)</a> and <a href=#m>machine learning (ML)</a>. With data exploration, analysts attempt to find patterns and details in large pools of data. Data exploration uses a mix of different manual and automated techniques and processes. Its function is not to sort all the <a href=#d>data</a>, but rather look specifically for the broad picture strokes that are evident within the data.</p></li><li><p><strong>deep model:</strong> a type of <a href=#n>neural network</a> containing multiple hidden layers.</p></li><li><p><strong>data analysis:</strong> obtaining an understanding of <a href=#d>data</a> by considering samples, measurement, and visualization. Data analysis can be particularly useful when a <a href=#d>dataset</a> is first received, before one builds the first model. It is also crucial in understanding experiments and debugging problems with the system.</p></li></ul><hr><h2 id=e>E</h2><ul><li><p><strong>ethnicity estimation:</strong> ethnicity estimation refers to classifying individuals based on perceived ethnic and or racial groups. Ethnicity and racial classifications systems are rife with subjectivity, often having ethnicity and racial labels applied by crowdsourced <a href=#a>annotation</a> workers who are never able to know the ground truth and only provide a subjective <a href=#c>classification</a>.</p></li><li><p><strong>epoch:</strong> in the context of training <a href=#d>deep Learning (DL)</a> models, one pass of the full <a href=#t>training data set</a>.</p></li><li><p><strong>embeddings:</strong> a categorical feature represented as a continuous-valued feature. Typically, an embedding is a translation of a high-dimensional vector into a low-dimensional space.</p></li></ul><hr><h2 id=f>F</h2><ul><li><p><strong>face recognition:</strong> face recognition refers to a system of <a href=#a>algorithms</a> that compares the similarity of two face images and provides a similarity score based on the distance between two face vectors. A face vector is a high-dimensional representation of face descriptors used to describe the features of a face that make it separable from other faces. The face vector is unique to the network, not the face.</p></li><li><p><strong>face detection:</strong> face detection is a type of <a href=https://en.wikipedia.org/wiki/Object_detection>object detection</a> that detects a single object class (a face). It is important to understand that detection and recognition are entirely separate algorithms. A <a href=#f>face recognition</a> system is a software application that uses <a href=#f>face detection</a> to locate a face followed by <a href=#f>face-alignment</a> to normalize the face position, then runs the cropped and aligned face chip through a <em>“face recognition”</em> network to compute its feature embedding.</p></li><li><p><strong>facial recognition system:</strong> a system that uses <a href=#f>facial recognition software</a>.</p></li><li><p><strong>face landmarks:</strong> face landmarks refer to predefined facial positions, for example the left corner of the left eye. Face landmarking algorithms compute <a href=#b>biometric</a> information, though it is primary not used to identify an individual, rather to perform <a href=#f>face alignment</a> prior to <a href=#f>face recognition</a>.</p></li><li><p><strong>face embeddings:</strong> an array of floating numbers that represent the values of facial features. Face embeddings can be thought of as face adjectives. Typically face-embeddings are between 128-4096 <em>“adjectives”</em> long. A longer number does necessarily correlate to higher performance.</p></li><li><p><strong>facial recognition software:</strong> software used to compare the visible physical structure of an individual’s face with a stored <a href=#f>facial template</a>.</p></li><li><p><strong>face alignment:</strong> face alignment is a <a href=#c>computer vision (CV)</a> technology for identifying the geometric structure of human faces in digital images. Given the location and size of a face, it automatically determines the shape of the face components such as eyes and nose. A face alignment program typically operates by iteratively adjusting a deformable models, which encodes the prior knowledge of face shape or appearance, to take into account the low-level image evidences and find the face that is present in the image.</p></li><li><p><strong>feature:</strong> an input variable used in making <a href=#p>predictions</a>.</p></li><li><p><strong>faceprinting:</strong> a fundamental step in the process of <a href=#f>face recognition</a>, faceprinting is the automated analysis and translation of visible characteristics of a face into a unique mathematical representation of that face. Both collection and storage of this information raise privacy and safety concerns.</p></li><li><p><strong>face matching:</strong> any comparison of two or more <a href=#f>faceprints</a>. This includes <a href=#f>face identification</a>, <a href=#f>face verification</a>, <a href=#f>face clustering</a>, and <a href=#f>face tracking</a>.</p></li><li><p><strong>face identification:</strong> compares (i) a single <a href=#f>faceprint</a> of an unknown person to (ii) a set of faceprints of known people. The goal is to identify the unknown person. Face identification may yield multiple results, sometimes with a <em>&ldquo;confidence&rdquo;</em> indicator showing how likely the system determines the returned image matches the unknown image.</p></li><li><p><strong>face verification:</strong> compares (i) a single <a href=#f>faceprint</a> of a person seeking verification of their authorization to (ii) one or more faceprints of authorized individuals. The verified person might or might not be identified as a specific person; a system may verify that two faceprints belong to the same person without knowing who that person is. Face verification may be used to unlock a phone or to authorize a purchase.</p></li><li><p><strong>facial template:</strong> a digital representation of distinct characteristics of a Subject’s face, representing information extracted from a photograph using a <a href=#f>facial recognition</a> <a href=#a>algorithm</a>.</p></li><li><p><strong>facial image:</strong> a photograph or video frame or other image that shows the visible physical structure of an individual’s face.</p></li><li><p><strong>face clustering:</strong> compares all the <a href=#f>faceprints</a> in a collection of images to one another, in order to group the images containing a particular person or group of people. The clustered people might or might not then be identified as known individuals. For example, each of the people in a library of digital photos (whether a personal album or a police array of everyone at a protest) could have their various pictures automatically clustered into a discrete set.</p></li><li><p><strong>face tracking:</strong> uses <a href=#f>faceprints</a> to follow the movements of a particular person through a physical space covered by one or more surveillance cameras, such as the interior of a store or the exterior sidewalks in a city’s downtown. The tracked person might or might not be identified. The tracking might be real-time or based on historical footage.</p></li><li><p><strong>face analysis, also known as face inference:</strong> any processing of a <a href=#f>faceprint</a>, without comparison to another individual’s faceprint, to learn something about the person from whom the faceprint was extracted. Face analysis by itself will not identify or verify a person. Some face analysis purports to draw inferences about a person’s demographics (such as race or gender), emotional or mental state (such as anger), behavioral characteristics, and even criminality.</p></li><li><p><strong>facial recognition data:</strong> data derived from the application of <a href=#f>facial recognition software</a>, including <a href=#f>facial template</a> and associated <a href=#m>metadata</a>.</p></li></ul><hr><h2 id=g>G</h2><ul><li><strong>graphics processing unit (GPU):</strong> a specialized hardware device used in computers, smartphones, and embedded systems originally built for real-time computer graphics rendering. However, the ability of GPUs to efficiently process many inputs in parallel has made them useful for a wide range of applications—including training <a href=#a>AI</a> <a href=#m>models</a>.</li></ul><hr><h2 id=h>H</h2><ul><li><strong>hash:</strong> the result of a mathematical function known as a <em>“hash function”</em> that converts arbitrary data into a unique (or nearly unique) numerical output. In <a href=#f>facial authentication</a>, for example, a complex hash function encodes the identifying characteristics of a user’s face and returns a numerical result. When a user attempts to access the system, their face is rehashed and compared with existing hashes to verify their identity.</li></ul><hr><h2 id=i>I</h2><ul><li><p><strong>image quality control:</strong> the use of <a href=#a>AI</a> and <a href=#m>machine learning (ML)</a> to perform automatic quality control on visual data, such as images and videos. For example, image quality control tools can detect image defects such as blurriness, nudity, <a href=https://en.wikipedia.org/wiki/Deepfake>deepfakes</a>, and banned content, and correct the issue or delete the image from the <a href=#d>dataset</a>.</p></li><li><p><strong>image recognition:</strong> a subfield of <a href=#a>AI</a> and <a href=#c>computer vision (CV)</a> that seeks to recognize the contents of an image by describing them at a high level. For example, a trained image recognition model might be able to distinguish between images of dogs and images of cats. Image recognition is contrasted with image segmentation, which seeks to divide an image into multiple parts (e.g. the background and different objects).</p></li><li><p><strong>image segmentation:</strong> a subfield of <a href=#c>computer vision (CV)</a> that seeks to divide an image into contiguous parts by associating each pixel with a certain category, such as the background or a foreground object.</p></li><li><p><strong>implicit bias:</strong> automatically making an association or assumption based on one’s mental models and memories. Implicit bias can affect the following: How <a href=#d>data</a> is collected and <a href=#c>classified</a>. How <a href=#m>machine learning (ML)</a> systems are designed and developed.</p></li></ul><hr><h2 id=j>J</h2><ul><li><strong>json response:</strong> a response to an API request that uses the popular and lightweight <a href=https://www.json.org/json-en.html>JSON (JavaScript Open Notation) file format</a>. A JSON response consists of a top-level array that contains one or more key-value pairs (e.g. <code>{ “name”: “John Smith”, “age”: 30 }</code>).</li></ul><hr><h2 id=l>L</h2><ul><li><p><strong>labeling:</strong> the process of assigning a label that provides the correct context for each input in the <a href=#t>training dataset</a>, or the <em>“answer”</em> that you would like the <a href=#a>AI</a> model to return during training. In <a href=#c>computer vision (CV)</a>, there are two types of labeling: <a href=#a>annotation</a> and <a href=#t>tagging</a>. Labeling can be performed in-house or through outsourcing or crowdsourcing services.</p></li><li><p><strong>liveness detection:</strong> a security feature for <a href=#f>facial authentication</a> systems to verify that a given image or video represents a live, authentic person, and not an attempt to fraudulently bypass the system (e.g. by wearing a mask of a person’s likeness, or by displaying a sleeping person’s face).</p></li></ul><hr><h2 id=m>M</h2><ul><li><p><strong>machine learning (ML):</strong> machine learning or ML for short – is a subset of <a href=#a>AI</a>. A program or system that builds (trains) a predictive model from input <a href=#d>data</a>. The system uses the learned model to make useful <a href=#p>predictions</a> from new (neverbefore-seen) data drawn from the same distribution as the one used to train the model. Machine learning (ML) also refers to the field of study concerned with these programs or systems.</p></li><li><p><strong>metadata:</strong> data that describes and provides information about other <a href=#d>data</a>. For visual data such as images and videos, metadata consists of three categories: technical (e.g. the camera type and settings), descriptive (e.g. the author, date of creation, title, contents, and keywords), and administrative (e.g. contact information and copyright).</p></li><li><p><strong>model:</strong> the representation of what a <a href=#m>machine learning (ML)</a> system has learned from the <a href=#t>training data</a>.</p></li></ul><hr><h2 id=n>N</h2><ul><li><p><strong>neural network:</strong> a neural network is a series of <a href=#a>algorithms</a> that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature.</p></li><li><p><strong>neural network model:</strong> model refers to the trained neural network file. This can be a single file or multiple files that defined the parameters of the final neural network. Often these files are several hundred megabytes in size while smaller, optimized versions can be only a few megabytes.</p></li><li><p><strong>noise:</strong> mislabeled data points, misrecorded or omitted feature values are all examples of noise. Essentially, anything that interferes with a clean and consistent <a href=#d>dataset</a> is considered noise.</p></li></ul><hr><h2 id=p>P</h2><ul><li><p><strong>pre-trained model:</strong> an <a href=#a>AI</a> <a href=#m>model</a> that has already been trained on a set of input <a href=#t>training data</a>. Given an input, a pre-trained model can rapidly return its prediction on that input, without needing to train the model again. Pre-trained models can also be used for <a href=#t>transfer learning</a>, i.e. applying knowledge to a different but similar problem (for example, from recognizing car manufacturers to truck manufacturers).</p></li><li><p><strong>polygon:</strong> this one is a usually non-rectangular shape outlining the object of interest and allowing more precision than a regular <a href=#b>bounding box</a>.</p></li><li><p><strong>prediction:</strong> a <a href=#m>model&rsquo;s</a> output when provided with an input example.</p></li><li><p><strong>precision:</strong> the number of correct positive results divided by the number of all positive results returned by a classifier.</p></li><li><p><strong>prediction bias:</strong> a value indicating how far apart the average of <a href=#p>predictions</a> is from the average of labels in the <a href=#d>dataset</a>. Not to be confused with the bias term in <a href=#m>machine learning models</a> or with <a href=#b>bias</a> in ethics and fairness.</p></li></ul><hr><h2 id=r>R</h2><ul><li><p><strong>recurrent neural network (RNN):</strong> a special type of <a href=#n>neural network</a> that uses the output of the previous step as the input to the current step. RNNs are best suited for sequential and time-based data such as text and speech.</p></li><li><p><strong>reinforcement learning:</strong> reinforcement learning is a type of <a href=#a>algorithm</a> that continuously mines feedback from previous iterations, learns on trial and error, and is led by the action-reward principle. In games, reinforcement learning algorithms are often used to analyze historical data and discover sequences that eventually lead either to victory or defeat.</p></li><li><p><strong>regression:</strong> regression is a <a href=#s>supervised learning</a> approach with continuous target variables. In regression tasks, we evaluate the performance of <a href=#m>machine learning (ML)</a> <a href=#a>algorithms</a> based on how close the predicted values are to the actual values.</p></li></ul><hr><h2 id=s>S</h2><ul><li><p><strong>supervised learning:</strong> supervised learning is an approach to creating <a href=#a>AI</a>, where a computer <a href=#a>algorithm</a> is trained on input data that has been labeled for a particular output. In other words, there is a <em>“supervisor”</em>, e.g., data annotator, who labels the training data points for future deployment.</p></li><li><p><strong>scraping images:</strong> <em>“scraping”</em> typically refers to obtaining images through technical methods not explicitly provided by a website. Using custom software to parse a website’s HTML and download the images, or using a web-browser emulator render a page and record visual elements from a virtual webpage render can be considered scraping. Downloading images through an interface or <a href=https://en.wikipedia.org/wiki/API>API (Application Programming Interface)</a> provided by the website is not typically considered scraping. For example obtaining images from search engine results would typically be considered scraping while obtaining images through the Flickr API would be considered downloading. However, when scripts or custom software is used to rotate API keys, IP addresses, and user-agents to avoid rate-limiting then it could be considered “scraping”.</p></li><li><p><strong>structured data:</strong> <a href=#d>data</a> that adheres to a known, predefined schema, making it easier to query and analyze.</p></li><li><p><strong>similarity thresholding:</strong> the process of converting a numerical similarity score measured between two face templates into a match or no-match determination. This typically involves a single static similarity threshold, such that any similarity score lower than the threshold is determined to be a no-match, and any similarity score greater than the <a href=#t>threshold</a> is determined to be a match.</p></li></ul><hr><h2 id=t>T</h2><ul><li><p><strong>training dataset:</strong> the portion of the <a href=#d>dataset</a> used to train an <a href=#a>algorithm</a>, during which a <a href=#n>neural network</a> learns weights and features that are later encoded into a <a href=#m>model</a>.</p></li><li><p><strong>test dataset:</strong> a portion of a <a href=#d>dataset</a> used to evaluate the algorithm’s performance. Often the test dataset is approximately 20% split of the full dataset, but it can be a completely separate standalone dataset. In this case it would be referred to as a <a href=#b>benchmark dataset</a>.</p></li><li><p><strong>tagging:</strong> the process of <a href=#l>labeling</a> the input data with a single tag in preparation for <a href=#a>AI</a> training. <a href=#t>Tagging</a> is similar to <a href=#a>annotation</a>, but uses only a single label for each piece of input data. For example, if you want to perform <a href=#i>image recognition</a> for different dog breeds, your tags may be <em>“golden retriever”</em>, <em>“bulldog”</em>, etc.</p></li><li><p><strong>transfer learning:</strong> a <a href=#m>machine learning (ML)</a> technique that reuses a model trained for one problem on a different but related problem, shortening the training process. For example, transfer learning could apply a <a href=#m>model</a> trained to recognize car makes and models to identify trucks instead.</p></li><li><p><strong>threshold:</strong> a user setting for <a href=#f>facial recognition</a> systems for <a href=#f>authentication</a>, <a href=#f>verification</a> or <a href=#f>identification</a>. The acceptance or rejection of a <a href=#f>facial template</a> match is dependent on the match score falling above or below the threshold. The threshold is adjustable within the facial recognition system.</p></li></ul><hr><h2 id=u>U</h2><ul><li><p><strong>unsupervised learning:</strong> contrary to <a href=#s>supervised learning</a>, unsupervised learning does not involve human-suggested labels. It discovers the underlying structure or patterns among the data points by means of finding similarities or differences in information and <a href=#c>clustering</a> it.</p></li><li><p><strong>unstructured data:</strong> <a href=#d>data</a> that does not adhere to a predefined schema, making it more flexible but harder to analyze. Examples of unstructured data include text, images, and videos.</p></li></ul><hr><h2 id=v>V</h2><ul><li><strong>validation dataset:</strong> a portion of a <a href=#d>dataset</a> used to validate the training process. After each <a href=#e>epoch</a> the validation data is used to help determine if the training progress is moving in the right direction. Often the validation dataset is approximately 20% split of the full dataset with no overlap.</li></ul><hr><div class=footer>Powered by <a href=https://gohugo.io/>Hugo</a> with
<a href=https://github.com/mrmierzejewski/hugo-theme-console/>Console Theme</a>.</div></div></body></html>