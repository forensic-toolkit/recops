<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>recops/recops/documentation/functionality/</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><link rel=stylesheet href=https://forensic-toolkit.github.io/recops/hugo-theme-console/css/terminal-0.7.2.min.css><link rel=stylesheet href=https://forensic-toolkit.github.io/recops/hugo-theme-console/css/animate-4.1.1.min.css><link rel=stylesheet href=https://forensic-toolkit.github.io/recops/hugo-theme-console/css/console.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><meta property="og:title" content="Functionality"><meta property="og:description" content><meta property="og:type" content="article"><meta property="og:url" content="https://forensic-toolkit.github.io/recops/documentation/functionality/"><meta property="article:published_time" content="2022-01-21T00:00:00+00:00"><meta name=twitter:title content="Functionality"><meta name=twitter:description content="In this page we walk through recops core design and functionality. recops is mainly a command-line interface (CLI) toolkit and well documented, use --help to get description of each function and the available options."></head><body class=terminal><div class=container><div class=terminal-nav><header class=terminal-logo><div class="logo terminal-prompt"><a href=https://forensic-toolkit.github.io/recops/ class="no-style site-name">recops</a>:~#
<a href=https://forensic-toolkit.github.io/recops/documentation>documentation</a>/<a href=https://forensic-toolkit.github.io/recops/documentation/functionality>functionality</a>/</div></header><nav class=terminal-menu><ul vocab="https://schema.org/" typeof="BreadcrumbList"><li><a href=https://forensic-toolkit.github.io/recops/about/ typeof="ListItem">about/</a></li><li><a href=https://forensic-toolkit.github.io/recops/overview/ typeof="ListItem">overview/</a></li><li><a href=https://forensic-toolkit.github.io/recops/documentation/ typeof="ListItem">documentation/</a></li></ul></nav></div></div><div class="container animated zoomIn fast"><h1>Functionality</h1><p>In this page we walk through <code>recops</code> core design and functionality. <code>recops</code> is mainly a command-line interface (CLI) toolkit and well documented, use <strong><code>--help</code></strong> to get description of each function and the available options.</p><pre tabindex=0><code>recops --help
recops &lt;function&gt; --help
</code></pre><p>This document explains <code>recops</code> internals, and we suggest reading to get a better understanding. Please keep in mind that this is a BETA version, is experimental, and <strong>not recommended</strong> for production workflows.</p><hr><h2 id=dataset>DATASET</h2><p>Dataset is the central class where all other classes (face(s), image(s), &ldquo;identity (ies)&rdquo;) are linked to. Keep in mind that you can create multiple datasets but any operation runs on a single dataset.</p><p>When we create a dataset we specify the relevant face detectors and face recognition models that will be used for all operations on this dataset. <code>recops</code> <strong>does not develop any model</strong>, is a wrapper around the open-sourced <a href=https://github.com/serengil/deepface>deepface</a> library from which it integrates eight face recognition models and five cutting-edge face detectors.</p><p>Face detection is a type of object detection that detects a single object class (a face). Currently, multiple cutting-edge facial detectors are wrapped in <code>recops</code>, you can choose between: <a href=https://opencv.org/>opencv</a>, <a href=https://github.com/code-cse/Face-Detection-SSD>ssd</a>, <a href=https://github.com/ipazc/mtcnn>mtcnn</a>, <a href=https://paperswithcode.com/paper/dlib-ml-a-machine-learning-toolkit>dlib</a>, <a href=https://paperswithcode.com/paper/190500641>retinaface</a> and <a href=https://google.github.io/mediapipe/>mediapipe</a>.</p><p>How to choose which <a href=../../overview/glossary/#f>face detection</a> model to use? We will try to briefly give you a good idea of when you should be using each model, allowing you to balance speed, accuracy, and efficiency. So, if your priority is to achieve high face detection accuracy, then you should consider using <code>retinaface</code> (the performance is very satisfactory as seen in the <a href=chuttersnap-8I423fRMwjM-unsplash.jpg>illustration</a> (image <a href=https://unsplash.com/photos/8I423fRMwjM>source</a>)) or <code>mtcnn</code> as a face detector. <code>retinaface</code> and <code>mtcnn</code> seem to overperform in detection and alignment stages but they are much slower.</p><p>On the other hand, if high speed is more important for your project, then you should use <code>dlib</code>, (<code>dlib</code> does not detect faces smaller than 80x80 so if working with small images make sure that you upscale them but this will increase the processing time) <code>opencv</code> or <code>ssd</code>.</p><p><a href=../../overview/glossary/#f>Facial recognition</a> is the task of making a positive identification of a face in a photo or video image against a pre-existing database of faces. It begins with detection - distinguishing human faces from other objects in the image - and then works on identification of those detected faces.</p><p>The following face recognition models can be used with the <code>recops</code> library. Most of them are based on <a href=../../overview/glossary/#c>Convolutional Neural Networks (CNN)</a> and provide best-in-class results: <a href=https://paperswithcode.com/paper/deep-face-recognition>VGG-Face</a>, <a href=https://paperswithcode.com/paper/facenet-a-unified-embedding-for-face>FaceNet</a> (Google), <a href=https://paperswithcode.com/paper/openface-a-general-purpose-face-recognition>OpenFace</a>, <a href=https://paperswithcode.com/paper/deepface-closing-the-gap-to-human-level-1>DeepFace</a> (Facebook), <a href=https://paperswithcode.com/paper/deep-learning-face-representation-from-1>DeepID</a>, <a href=https://paperswithcode.com/paper/arcface-additive-angular-margin-loss-for-deep>ArcFace</a>, <a href=https://paperswithcode.com/paper/dlib-ml-a-machine-learning-toolkit>Dlib</a> and <a href=https://paperswithcode.com/paper/sface-sigmoid-constrained-hypersphere-loss-1>SFace</a>. In the table below you can find out the scores of face recognition models on <a href=http://vis-www.cs.umass.edu/lfw/>Labeled Faces in the Wild</a> dataset.</p><table><thead><tr><th><strong>Model</strong></th><th><strong>Task</strong></th><th><strong>Dataset</strong></th><th><strong>Metric Name</strong></th><th><strong>Metric Value</strong></th><th><strong>Rank</strong></th><th><strong>Paper</strong></th><th><strong>Code</strong></th><th><strong>Year</strong></th></tr></thead><tbody><tr><td>ArcFace</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href="https://paperswithcode.com/paper/arcface-additive-angular-margin-loss-for-deep/review/?hl=2259">99.83%</a></td><td># 1</td><td><a href=https://paperswithcode.com/paper/arcface-additive-angular-margin-loss-for-deep>ArcFace: Additive Angular Margin Loss for Deep Face Recognition</a></td><td><a href=https://github.com/deepinsight/insightface>GitHub</a></td><td>2018</td></tr><tr><td>Facenet512</td><td>-</td><td>-</td><td>-</td><td>99.65%</td><td># 2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Facenet</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href="https://paperswithcode.com/paper/facenet-a-unified-embedding-for-face/review/?hl=2252">99.63%</a></td><td># 3</td><td><a href=https://paperswithcode.com/paper/facenet-a-unified-embedding-for-face>FaceNet: A Unified Embedding for Face Recognition and Clustering</a></td><td><a href=https://github.com/davidsandberg/facenet>GitHub</a></td><td>2015</td></tr><tr><td>SFace</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href=https://paperswithcode.com/paper/sface-sigmoid-constrained-hypersphere-loss-1#results>99.60%</a></td><td># 4</td><td><a href=https://paperswithcode.com/paper/sface-sigmoid-constrained-hypersphere-loss-1>SFace: Sigmoid-Constrained Hypersphere Loss for Robust Face Recognition</a></td><td><a href=https://github.com/zhongyy/SFace>GitHub</a></td><td>2022</td></tr><tr><td>Dlib</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href=https://paperswithcode.com/paper/dlib-ml-a-machine-learning-toolkit#results>99.38%</a></td><td># 5</td><td><a href=https://paperswithcode.com/paper/dlib-ml-a-machine-learning-toolkit>Dlib-ml: A Machine Learning Toolkit</a></td><td><a href=https://github.com/davisking/dlib>GitHub</a></td><td>2009</td></tr><tr><td>VGG-Face</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href=https://paperswithcode.com/paper/deep-face-recognition#results>98.78%</a></td><td># 6</td><td><a href=https://paperswithcode.com/paper/deep-face-recognition>Deep Face Recognition</a></td><td><a href=https://github.com/serengil/deepface>GitHub</a></td><td>2015</td></tr><tr><td>DeepFace</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href=https://paperswithcode.com/paper/deepface-closing-the-gap-to-human-level-1#results>98.37%</a></td><td># 7</td><td><a href=https://paperswithcode.com/paper/deepface-closing-the-gap-to-human-level-1>DeepFace: Closing the Gap to Human-Level Performance in Face Verification</a></td><td><a href=https://github.com/swghosh/DeepFace>GitHub</a></td><td>2014</td></tr><tr><td>Human Beings</td><td>-</td><td>-</td><td>-</td><td>97.53%</td><td># 8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>DeepID</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href=https://paperswithcode.com/paper/deep-learning-face-representation-from-1#results>97.05%</a></td><td># 9</td><td><a href=https://paperswithcode.com/paper/deep-learning-face-representation-from-1>Deep Learning Face Representation from Predicting 10,000 Classes</a></td><td><a href=https://github.com/serengil/deepface>GitHub</a></td><td>2014</td></tr><tr><td>OpenFace</td><td>Face Verification</td><td>Labeled Faces in the Wild</td><td>Accuracy</td><td><a href=https://paperswithcode.com/paper/openface-a-general-purpose-face-recognition#results>92.92%</a></td><td># 10</td><td><a href=https://paperswithcode.com/paper/openface-a-general-purpose-face-recognition>OpenFace: A general-purpose face recognition library with mobile applications</a></td><td><a href=https://github.com/cmusatyalab/openface>GitHub</a></td><td>2016</td></tr></tbody></table><p>Additionally dataset holds information on the distance-metric. Is the method being used to calculate the distance between two faces vectors and can be: <code>cosine</code>, <code>euclidean</code>, <code>euclidean_l2</code>. The default configuration uses <code>cosine</code>.</p><p><em>NOTE: Performing <a href=../../overview/glossary/#f>face alignment</a> for facial recognition can dramatically improve performance. Experiments show that just alignment increases the face recognition accuracy almost 1%. Face alignment is an early stage of the modern <a href="https://scontent.fath6-1.fna.fbcdn.net/v/t39.8562-6/240890413_887772915161178_4705912772854439762_n.pdf?_nc_cat=109&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=dnukaiC5BiwAX_ikCeA&_nc_ht=scontent.fath6-1.fna&oh=00_AT-YrnWvoT1Fc2cGymsk2tUBUeZo2otLGBryeni6MMZ-6w&oe=63504DBF">face recognition pipeline</a>. Google <a href=https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/poster.pdf>declared</a> that face alignment increases the accuracy of its face recognition model <a href=https://paperswithcode.com/paper/facenet-a-unified-embedding-for-face>FaceNet</a> from 98.87% to 99.63%. This is almost 1% accuracy improvement.</em></p><p><strong>Following are the command-line interface (CLI) functions relevant to dataset</strong></p><p><em>NOTE: Before running any command please set your <code>DATABASE_URI</code> and <code>STORAGE_URI</code> environment variables. Please read <a href=../installation/>&ldquo;Installation&rdquo;</a> carefully if you havent already.</em></p><p>To create or update a dataset use the following. Keep in mind when updating a dataset <code>--detector</code> and <code>--basemodel</code> options can&rsquo;t change.</p><pre tabindex=0><code>recops dataset-create --detector $DETECTOR_OPTION --basemodel $BASEMODEL_OPTION $DATASET_NAME
</code></pre><ul><li><strong><code>--detector</code></strong>, to select face detection model. Default: <code>retinaface</code></li></ul><pre tabindex=0><code>Available options = [&#34;opencv&#34;, &#34;ssd&#34;, &#34;mtcnn&#34;, &#34;dlib&#34;, &#34;retinaface&#34;, &#34;mediapipe&#34;]
</code></pre><ul><li><strong><code>--basemodel</code></strong>, to select face recognition model. Default: <code>ArcFace</code></li></ul><pre tabindex=0><code>Available options = [&#34;VGG-Face&#34;, &#34;Facenet&#34;, &#34;Facenet512&#34;, &#34;OpenFace&#34;, &#34;DeepFace&#34;, &#34;DeepID&#34;, &#34;ArcFace&#34;, &#34;Dlib&#34;, &#34;SFace&#34;]
</code></pre><ul><li><p><strong><code>--threshold</code></strong>, maximum distance to be used when comparing faces (unless specified the default threshold of basemodel is used).</p></li><li><p><strong><code>--distance-metric</code></strong>, metric to compute distance when comparing faces.</p></li></ul><pre tabindex=0><code>Available options = [&#34;cosine&#34;, &#34;euclidean&#34;, &#34;euclidean_l2&#34;]
</code></pre><p>To list all available dataset(s) and additional information use:</p><pre tabindex=0><code>recops dataset-list
</code></pre><p>Dataset output information:</p><pre tabindex=0><code>&lt;Dataset[ 
    id: {self.id}
    name: {self.name}
    detector: {self.detector_backend}
    basemodel: {self.basemodel_backend}
    identities: {self.identity_count}
    faces: {self.face_count} 
    threshold: {self.threshold}
    default_threshold: {self.default_threshold}
    distance_metric: {self.distance_metric}
]&gt;
</code></pre><p>To list available face(s), image(s), &ldquo;identity (ies)&rdquo; within a dataset use:</p><pre tabindex=0><code># List available faces in dataset
recops dataset-list-faces -d $DATASET_ID

# List available images in dataset
recops dataset-list-images -d $DATASET_ID

# List available identities in dataset
recops dataset-list-identities -d $DATASET_ID
</code></pre><p>Export datasets content into a ZIP archive:</p><pre tabindex=0><code>recops dataset-export -d $DATASET_ID -o $PATH_TO_SAVE_ARCHIVE
</code></pre><p>To delete a specified dataset and all faces, images and identities linked to it use:</p><pre tabindex=0><code>recops dataset-delete $DATASET_ID
</code></pre><hr><h2 id=face>FACE</h2><p>The word <em>“face”</em> generally refers to the front-most region of the uppermost part of the human body. In <code>recops</code> context face is a class contains information on a particular face and it is always derived from an image, and linked to it. (Please read <a href=#import-data>&ldquo;IMPORT DATA&rdquo;</a> section on how to import face(s))</p><p>To delete a face use:</p><pre tabindex=0><code>recops face-delete $FACE_ID
</code></pre><hr><h2 id=image>IMAGE</h2><p>Image stands for an image file and is always linked to that file. Keep in mind that any import, or recognition operation will create image(s) and the files will be copied to <code>STORAGE_URI</code>. After the import you can delete the files being used during the import as they are now contained in <code>STORAGE_URI</code>.</p><p>The image class holds the checksum of the related file and before any import it checks if image is already imported by comparing the checksums. (Please read <a href=#import-data>&ldquo;IMPORT DATA&rdquo;</a> section below on how to import image(s))</p><p>To delete an image use:</p><pre tabindex=0><code>recops image-delete $IMAGE_ID
</code></pre><hr><h2 id=identity>IDENTITY</h2><p>Identity does not have to be a real identity, but rather a group of faces. Depending on the investigation or case study, the group may refer to person’s name, job, team or anything. It can link to a single or multiple faces.</p><p>Create an identity:</p><pre tabindex=0><code>recops identity-create -d $DATASET_ID $IDENTITY_NAME
</code></pre><p>Link face to an identity:</p><pre tabindex=0><code>recops identity-link $IDENTITY_ID $FACE_ID
</code></pre><p>To delete an identity use:</p><pre tabindex=0><code>recops identity-delete $IDENTITY_ID
</code></pre><hr><h2 id=import-data>IMPORT DATA</h2><p>The process of importing data is the most important and sometimes confusing. <code>recops</code> has 3 different functions to help with that:</p><pre tabindex=0><code># Import face(s) from local folder
recops dataset-import-faces --help

# Import face(s) linked to &#34;identity (ies)&#34; from local folder
recops dataset-import-identities --help

# Import image(s) to dataset from a local folder
recops dataset-import-images --help
</code></pre><p><code>recops</code> handles 5 common stages in the background: <strong>detect</strong>, <strong>align</strong>, <strong>normalize</strong>, <strong>represent</strong> and <strong>verify</strong>. Before going into details about each operation we will first explain how the face extraction process works that is common for all 3 imports.</p><p>Any import will handle at least one image file and follow this process:</p><ul><li><p>Compute checksum of given image and check if image is already stored, otherwise store it.</p></li><li><p>It stores the image to <code>recops</code> <code>STORAGE_URI</code> and image information in database.</p></li><li><p>An image may hold many faces or none. We detect face(s) in the image using dataset&rsquo;s <code>--detector</code> options.</p></li></ul><p>For each detected face:</p><ul><li><p>We link it to the image originated from (check if the detected face is already stored (based on its checksum) otherwise it will store it).</p></li><li><p>Store it&rsquo;s location (<em>&ldquo;box_left&rdquo;</em>, <em>&ldquo;box_top&rdquo;</em>, <em>&ldquo;box_right&rdquo;</em>, <em>&ldquo;box_bottom&rdquo;</em>).</p></li><li><p><code>recops</code> follows <a href=https://github.com/serengil/deepface>deepface</a> procedures and extract <a href=../../overview/glossary/#f>face landmarks</a> from a detected face, preprocess and normalize the detected face before extracting the landmarks.</p></li><li><p>Then we extract and store the face vectors (<a href=../../overview/glossary/#f>face embeddings</a>), an array of floating numbers that represent the face and is unique for each face. The face vectors are computed using selected dataset&rsquo;s <code>--basemodel</code> options.</p></li></ul><p><em>NOTE: By default <code>recops</code> aligns faces, so if you already have pre-processed images and you would like to skip alignment use <code>--no-align</code> flag.</em></p><p>There always gonna be some edge cases that will fail. Keep in mind that when a face is detected but we fail to extract facial features, then we do not store it and not process it any further.</p><p>All import functions might be time consuming depending on how much data we process, the basemodel, and detector we use. While importing, it is safe to stop the process using <code>Ctrl-C</code> at any time. When you rerun the function, the import will start from where is left. If this is not desirable behaviour, you can use <code>--force</code> flag to recompute everything.</p><hr><h3 id=import-images>IMPORT IMAGE(S)</h3><p>Import image(s) to dataset from a local folder. This is a generic import, where all image(s) in a folder are processed and face(s) are extracted from image(s). This process will not link the face(s) to any &ldquo;identity (ies)&rdquo;, you should do it manually. The import will only handle <strong><code>.png</code></strong>, <strong><code>.jpeg</code></strong>, and <strong><code>.jpg</code></strong> files.</p><p>Example folder structure:</p><pre tabindex=0><code>├── local folder
│   ├── image-001.jpeg
│   ├── whatever-name.jpeg
│   ├── unrelated.pdf       &lt;= will be skipped
│   ├── unrelated.mp4       &lt;= will be skipped
│   ├── another-image.png
│   ├── whatever-name.gif   &lt;= will be skipped
</code></pre><p>Basic use of this command (use <code>dataset-import-images --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops dataset-import-images -d $DATASET_ID $PATH_TO_IMAGES
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>--consent</code></strong>, mark imported images/faces as have been consent to use.</li><li><strong><code>--force</code></strong>, force to reprocess existing objects.</li><li><strong><code>--align / --no-align</code></strong>, align detected faces.</li><li><strong><code>--output-objects</code></strong>, file path to write list of objects created (by default it doesnt output anything).</li><li><strong><code>--output-format</code></strong>, output format for <strong><code>--output-objects</code></strong> (default: <code>.csv</code>).</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h3 id=import-faces>IMPORT FACE(S)</h3><p>Import face(s) from local folder. Consider using this function when you already have a list of image(s) contain cropped and aligned face(s) (not a prerequisite). Each image should contain a single face, if no face or more than one face is detected then the process will print an error and will not import it (to see errors set &ndash;log-level to INFO).</p><p>In case you want to export all images that produced errors use <code>--output-errors</code> and point to a folder where error images will be copied.</p><p>If all faces are part of the same identity then you can use <code>--identity-id</code> and all faces will be linked to that identity (use <code>identity-create</code> to create an identity first).</p><p>Example folder structure:</p><pre tabindex=0><code>├── local folder
│   ├── face-001.jpeg
│   ├── face-002.jpeg
│   ├── whatever-name.jpeg
│   ├── blah.png
</code></pre><p>Basic use of this command (use <code>dataset-import-faces --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops dataset-import-faces -d $DATASET_ID $PATH_TO_FACES
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>--consent</code></strong>, mark imported images/faces as have been consent to use.</li><li><strong><code>--identity-id</code></strong>, link faces to specified identity (None by default).</li><li><strong><code>--align / --no-align</code></strong>, align detected faces.</li><li><strong><code>--force</code></strong>, force to reprocess existing objects.</li><li><strong><code>--output-errors</code></strong>, specify path to export files contain errors (by default it doesn&rsquo;t export anything).</li><li><strong><code>--output-objects</code></strong>, file path to write list of objects created (by default it doesnt output anything).</li><li><strong><code>--output-format</code></strong>, output format (default: <code>.csv</code>).</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h3 id=import-identity-ies>IMPORT &ldquo;IDENTITY (IES)&rdquo;</h3><p>Import face(s) linked to “identity (ies)” from local folder. Consider using this function when you already have a list of image(s) contain cropped and aligned face(s) (not a prerequisite), grouped in folders named after face(s) identity.</p><p>Each image should contain a single face, if no face or more than one face detected then the process will print an error and will not import it (to see errors set &ndash;log-level to INFO).</p><p>In case you want to export &ldquo;identity (ies)"/face(s) with errors use <code>--output-errors</code> to specify a folder where error images will be copied.</p><p>Folder structure should be in the following format:</p><pre tabindex=0><code>├── local folder
│   ├── identity-name-001
│   │   ├── face-001.jpeg
│   │   ├── face-002.jpeg
│   ├── identity-name-002
│   │   ├── face-001.jpeg
│   │   ├── face-002.jpeg
│   │   ├── face-003.jpeg
│   ├── identity-name-003
│   │   ├── face-001.jpeg
│   │   ├── face-002.jpeg
│   ├── identity-name-004
│   │   ├── face-001.jpeg
│   │   ├── face-002.jpeg
│   │   ├── face-003.jpeg
│   │   ├── face-004.jpeg
</code></pre><p>Basic use of this command (use <code>dataset-import-identities --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops dataset-import-identities -d $DATASET_ID $PATH_TO_IDENTITIES_AND_FACES
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>--consent</code></strong>, mark imported images/faces as have been consent to use.</li><li><strong><code>--align / --no-align</code></strong>, align detected faces.</li><li><strong><code>--force</code></strong>, force to reprocess existing objects.</li><li><strong><code>--output-errors</code></strong>, specify path to export files contain errors (by default it doesn&rsquo;t export anything).</li><li><strong><code>--output-objects</code></strong>, file path to write list of objects created (by default it doesnt output anything).</li><li><strong><code>--output-format'</code></strong>, output format (default: <code>.csv</code>).</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h2 id=backup>BACKUP</h2><p>A full database backup backs up the whole database and files, so that the full database can be recovered after a full database backup is restored. Full database backups represent the database at the time the backup finished.</p><p>To backup full database and files (by default outputs a <code>.zip</code> file) use:</p><pre tabindex=0><code>recops backup -o $PATH_TO_SAVE_ARCHIVE
</code></pre><p>To restore use the following:</p><pre tabindex=0><code>unzip backup.zip -d /tmp/data
export STORAGE_URI=file:///tmp/data
export DATABASE_URI=sqlite:////tmp/data/recops.db
recops dataset-list
</code></pre><p>Command options:</p><ul><li><strong><code>-o</code></strong>, <strong><code>--output</code></strong>, path to save archive (default: <code>output.zip</code>).</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h2 id=other-functionality>OTHER FUNCTIONALITY</h2><h2 id=match-faces>MATCH FACE(S)</h2><p>Compare face(s) linked to identity with face(s) without identity for given dataset. After importing your data, you might end up with unknown face(s) in your dataset and want to find if they match with identified face(s). In such case this function might help. This function will not change anything in the database it will just create a report that can be reviewed manually.</p><p>It will loop through face(s) without identity compare them with those having an identity. By <em>&ldquo;comparing faces&rdquo;</em> we mean that will compute the distance between their coresponding face vectors. Then we collect all matched face(s) that are equal to and below the specified threshold (use <code>--threshold</code> to specify that) and build a report.</p><p>The report will show a table with matched face(s), any coresponding “identity (ies)&rdquo; and their distance. After reviewing the report you can manually link the face(s) to the “identity (ies)" or can use the <code>dataset-link-matched-faces</code> function to do it for you in an automated fashion.</p><p>Use <code>--output</code> to specify file path to save results to, and <code>--output-format</code> to select the output file format, which can be either <code>.csv</code> or <code>.html</code>. The <code>.csv</code> file can be used to feed <code>dataset-link-matched-faces</code> function and the <code>.html</code> file can be used to go over it and check the faces manually, can be opened in any browser but for it to visualize the faces properly you need to run <code>webui</code> function.</p><p>Basic use of this command (use <code>dataset-match-faces --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops dataset-match-faces -d $DATASET_ID -o $PATH_TO_SAVE_OUTPUT
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>-o</code></strong>, <strong><code>--output</code></strong>, filename to export results (both <code>.csv</code> and <code>.html</code>).</li><li><strong><code>--threshold</code></strong>, maximum distance to be used when comparing faces (unless specified the default threshold of basemodel is used).</li><li><strong><code>--web-uri</code></strong>, web uri to use when formating <code>.html</code>.</li><li><strong><code>--exclude-identities</code></strong>, list of identities ids to exclude. Faces linked to these identities will not be included.</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h2 id=link-matched-faces>LINK MATCHED FACE(S)</h2><p>This is a helpful function to link face(s) to &ldquo;identity (ies)&rdquo; from given <code>.csv</code> compatible with export from <code>dataset-matched-faces</code> function. The <code>.csv</code> file should have at least the following two rows: <code>&lt;face_id>, &lt;identity_id></code></p><p>Basic use of this command (use <code>dataset-link-matched-faces --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops dataset-link-matched-faces $PATH_TO_CSV
</code></pre><hr><h2 id=cluster-faces>CLUSTER FACES</h2><p>Face clustering plays an essential role in exploiting massive unlabeled face data. Face clustering is a fundamental face analysis task and has wide applications in real-world scenarios like the dataset preparation or cleaning for face recognition.</p><p>Face recognition and <a href=../../overview/glossary/#f>face clustering</a> are different, but highly related concepts. When performing <a href=../../overview/glossary/#f>face recognition</a> we are applying <a href=../../overview/glossary/#s>supervised learning</a> where we have both example images of faces we want to recognize along with the names that correspond to each face (i.e., the <em>“class labels”</em>).</p><p>But in face clustering we need to perform <a href=../../overview/glossary/#u>unsupervised learning</a> — we have only the faces themselves with no names/labels. From there we need to identify and count the number of unique people in a dataset.</p><p><em>NOTE: This function might be time consuming depending on how much data we process. For truly massive datasets you should consider using the <a href=https://en.wikipedia.org/wiki/Chinese_whispers_(clustering_method)>chinese whispers algorithm</a> as it’s linear in time.</em></p><p>Use the following command to run face clustering for given dataset (use <code>dataset-cluster-faces --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops dataset-cluster-faces -d $DATASET_ID -o $PATH_TO_SAVE_OUTPUT
</code></pre><p>For debug logging and further analysis:</p><pre tabindex=0><code>recops --log-level DEBUG dataset-cluster-faces -d $DATASET_ID -o $PATH_TO_SAVE_OUTPUT
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>-o</code></strong>, <strong><code>--output</code></strong>, folder to export results.</li><li><strong><code>--threshold</code></strong>, maximum distance to be used when comparing faces (unless specified the default threshold of basemodel is used).</li><li><strong><code>--skip-single-faces</code></strong>, export only faces that match at least one face.</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h2 id=compute-extended-fields>COMPUTE EXTENDED FIELDS</h2><p>Moreover, <code>recops</code> comes with a strong facial attribute analysis module. You can use this function to analyze and extract rich metadata for each face in a dataset, including <a href=https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/>age</a>, <a href=https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/>gender</a>, <a href=https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/>facial expression</a> (including angry, fear, neutral, sad, disgust, happy and surprise) and <a href=https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/>race</a> (including asian, white, middle eastern, indian, latino and black) prediction.</p><p>Currently, the age model got ± 4.65 MAE; gender model got 97.44% accuracy, 96.29% precision and 95.05% recall as mentioned in this <a href=https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/>tutorial</a>. Also, the <a href=../../overview/glossary/#c>CNN</a> model to recognize facial expressions of human beings produces 57% accuracy on test set.</p><p>You can find here the entire source code for both <a href=https://github.com/serengil/tensorflow-101/blob/master/python/apparent_age_prediction.ipynb>apparent age prediction</a> and <a href=https://github.com/serengil/tensorflow-101/blob/master/python/gender_prediction.ipynb>gender prediction</a> as a notebook to GitHub. Similarly, code for <a href=https://github.com/serengil/tensorflow-101/blob/master/python/Race-Ethnicity-Prediction-Batch.ipynb>race and ethnicity prediction</a> and <a href=https://github.com/serengil/tensorflow-101/blob/master/python/facial-expression-recognition.py>facial expression recognition</a> on GitHub, too.</p><p><em>IMPORTANT NOTE: PLEASE KEEP IN MIND THAT THIS FUNCTION USES WEAK AND QUESTIONABLE MODELS THAT CATEGORIZE FACE(S) IN A COMPLETELY DISCRIMINATIVE WAY.</em></p><p>You can use the following command to execute the facial attribute analysis and test it out yourself (use <code>dataset-compute-extended-fields --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops dataset-compute-extended-fields -d $DATASET_ID
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>--age / --no-age</code></strong>, compute age field.</li><li><strong><code>--gender / --no-gender</code></strong>, compute gender field.</li><li><strong><code>--emotion / --no-emotion</code></strong>, compute emotion field.</li><li><strong><code>--race / --no-race</code></strong>, compute race field.</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h2 id=recognize>RECOGNIZE</h2><p>Run face recognition on a given image and get an output report. Once we have created a dataset, with face(s) linked (or not) to &ldquo;identity (ies)&rdquo;, we can run this function against a provided image to find out if the face(s) inside the image match any of the face(s)/“identity (ies)" in our dataset.</p><p>The function will run the following process:</p><p>First we detect the faces on the given image. By detecting the face we use detector model (as specified in dataset). Then for each face we use the basemodel (as specified in dataset) to extract its face embedding - or vectors - similar to what we discussed in <a href=#import-data>&ldquo;IMPORT DATA&rdquo;</a> section above. The face vectors are used as unique identifiers for the face.</p><p>Once we have the face vector we iterate through all faces that are stored in our dataset and compute the distance between the 2 vectors (for each detected and stored face pair). If the distance between the 2 face vectors, is equal to and below the specified <code>--threshold</code> then we consider it a match. The distance is computed by the distance metric as specified in dataset.</p><p>Then, we sort all matched faces (if any) with priority to the ones having smaller distance, and keep maximum 10 matched faces (by default, but you can use <code>--maximum</code> flag to specify more or less) that are closest to the detected face.</p><p>Last but not least we create a report out for our matches. The default report outputs an <code>.html</code> file. The file is self-contained, it doesnt use any external link, so it can be reviewed offline by any browser. It does that by including raw images encoded in base64 format and modern <a href=https://en.wikipedia.org/wiki/Scalable_Vector_Graphics>SVG (Scalable Vector Graphics)</a> functionality. It does not use any javascript. Additionally if the default report is not desired you can use your own template to customize how it looks like by specifying <code>--template</code>. To develop a template needs some development skills (nothing too special) and you should have a look at the existing template and follow it as a paradigm.</p><p>Use the following command to run face recognition on a given image (use <code>recognize --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops recognize -d $DATASET_ID $PATH_TO_IMAGE -o $PATH_TO_SAVE_OUTPUT
</code></pre><p>For debug logging and further analysis:</p><pre tabindex=0><code>recops --log-level DEBUG recognize --force -d $DATASET_ID $PATH_TO_IMAGE -o $PATH_TO_SAVE_OUTPUT
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>-o</code></strong>, <strong><code>--output</code></strong>, path to save output report.</li><li><strong><code>--threshold</code></strong>, maximum distance to be used when comparing faces (unless specified the default threshold of basemodel is used).</li><li><strong><code>--maximum</code></strong>, maximum number of matched faces to print (default: 10).</li><li><strong><code>--identified-faces</code></strong>, search only through faces with identity attached (default: False).</li><li><strong><code>--exclude-identities</code></strong>, comma seperated list of identity ids to exclude. Faces linked to these identities will not be used for recognition.</li><li><strong><code>--include-extended-fields</code></strong>, whether to include or not extended fiels in the result (default: False).</li><li><strong><code>--consent</code></strong>, mark imported images/faces as have been consent to use.</li><li><strong><code>--align / --no-align</code></strong>, align detected faces.</li><li><strong><code>--force</code></strong>, force to reprocess existing objects.</li><li><strong><code>--template</code></strong>, choose report template (default: <code>image-view.html.j2</code>).</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h2 id=verify>VERIFY</h2><p>This function verifies that 2 faces belongs to the same person or different persons. Actually, we only pass an image pair as an input, and that’s all! This procedure will not write anything in the database, will just detect the faces in the given input images, compare the similarity of two face images and provides a similarity score based on the distance between two face vectors.</p><p>The dataset is used to get the <code>--detector</code> and <code>--basemodel</code> and has no interaction with it&rsquo;s linked identities or faces whatsoever.</p><p>You can use the following command to verify face pairs as same person or different persons (use <code>verify --help</code> to list all of the available command options):</p><pre tabindex=0><code>recops verify $PATH_TO_IMAGE $PATH_TO_IMAGE -d $DATASET_ID
</code></pre><p>Command options:</p><ul><li><strong><code>-d</code></strong>, <strong><code>--dataset-id</code></strong>, specify dataset id.</li><li><strong><code>--threshold</code></strong>, maximum distance to be used when comparing faces (unless specified the default threshold of basemodel is used).</li><li><strong><code>--consent</code></strong>, mark imported images/faces as have been consent to use.</li><li><strong><code>--align / --no-align</code></strong>, align detected faces.</li><li><strong><code>--help</code></strong>, show this message and exit.</li></ul><hr><h2 id=webui>WEBUI</h2><p><code>recops</code> serves a web user-friendly interface as well, that allows you to easily manage basic operations. Is a standard base web application that connects via <code>HTTP</code> and allows enhanced productivity with direct access to structure data, easy browsing, and several other features built to provide a rich user experience.</p><p>To access the web interface, use the following command, launch a web browser and enter the server web address <a href=http://127.0.0.1:5000>http://127.0.0.1:5000</a>. Stop the process using <code>Ctrl-C</code> at any time.</p><pre tabindex=0><code>recops webui
</code></pre><p>Below are briefly described some of the basic <code>recops</code> operations served on the web user-friendly interface. You are expected to call these operations as <code>HTTP</code> post methods. Service endpoints will be:</p><ul><li><p><a href=http://127.0.0.1:5000/dataset/>http://127.0.0.1:5000/dataset/</a> edit or create new records, list available datasets, and additional information (name, description, detector backend, basemodel backend, threshold, distance metric, and more).</p></li><li><p><a href=http://127.0.0.1:5000/detected_face/>http://127.0.0.1:5000/detected_face/</a> this field serves the <code>reconize</code> function, you can edit records, search, and filter a list of all available detected faces from a given input image, additional relevant information, and more.</p></li><li><p><a href=http://127.0.0.1:5000/detect_image/>http://127.0.0.1:5000/detect_image/</a> this field serves the <code>reconize</code> function, you can edit records, search, and filter a list of all available input images, additional information, and more.</p></li><li><p><a href=http://127.0.0.1:5000/face/>http://127.0.0.1:5000/face/</a> edit records, search, and filter a list of all available faces contained in a dataset, additional information, and more (id, img array, gender, emotion, age, race, image.checksum, dataset, identities).</p></li><li><p><a href=http://127.0.0.1:5000/face_image/>http://127.0.0.1:5000/face_image/</a> this field serves the importing data operations, you can edit records, search, and filter a list of all available input images, additional relevant information, and more.</p></li><li><p><a href=http://127.0.0.1:5000/identity/>http://127.0.0.1:5000/identity/</a> this field serves the importing data operations, you can edit or create new records, search, and filter a list of all available “identity (ies)” contained in a dataset, additional information, and more.</p></li><li><p><a href=http://127.0.0.1:5000/matchedfaceassociation/>http://127.0.0.1:5000/matchedfaceassociation/</a> this field analyze the output result of <code>reconize</code> function, you can edit or create new records, search, and filter a list of detailed recordings, additional information, and more.</p></li></ul><p><code>recops</code> <code>webui</code> has been built with simplicity and composability in mind. Please keep in mind that this is a BETA version and the <code>webui</code> is in its testing phase. There are some known issues, a few missing features, and there are guaranteed to be bugs.</p><p>Our goal in this first release was to lay the foundation for a structured, extensible and integrated architecture. We’ll continue to add features for the foreseeable future and improve the performance of the user interface.</p><div class=footer>Powered by <a href=https://gohugo.io/>Hugo</a> with
<a href=https://github.com/mrmierzejewski/hugo-theme-console/>Console Theme</a>.</div></div></body></html>